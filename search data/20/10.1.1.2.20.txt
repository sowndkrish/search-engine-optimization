-Arc:

Ensemble Learning
in the Presence of Outliers
G. Ratsch

y

, B. Scholkopf

z

, A. Smola



,
K.-R. Muller

y

, T. Onoda

yy

, and S. Mika

y

y GMD FIRST, Rudower Chaussee 5, 12489 Berlin, Germany

z Microsoft Research, 1 Guildhall Street, Cambridge CB2 3NH, UK

 Dep. of Engineering, ANU, Canberra ACT 0200, Australia

yy CRIEPI, 2-11-1, Iwado Kita, Komae-shi, Tokyo, Japan

fraetsch, klaus, mikag@first.gmd.de, bsc@microsoft.com,
Alex.Smola@anu.edu.au, onoda@criepi.denken.or.jp
Abstract

AdaBoost and other ensemble methods have successfully been applied
to a number of classification tasks, seemingly defying problems
of overfitting. AdaBoost performs gradient descent in an error
function with respect to the margin, asymptotically concentrating
on the patterns which are hardest to learn. For very noisy problems,
however, this can be disadvantageous. Indeed, theoretical
analysis has shown that the margin distribution, as opposed to just
the minimal margin, plays a crucial role in understanding this phenomenon.
Loosely speaking, some outliers should be tolerated if
this has the benefit of substantially increasing the margin on the
remaining points. We propose a new boosting algorithm which allows
for the possibility of a pre-specified fraction of points to lie in
the margin area or even on the wrong side of the decision boundary.
1 Introduction

Boosting and related Ensemble learning methods have been recently used with great
success in applications such as Optical Character Recognition (e.g. [8, 16]).
The idea of a large minimum margin [17] explains the good generalization performance
of AdaBoost in the low noise regime. However, AdaBoost performs worse
on noisy tasks [10, 11], such as the iris and the breast cancer benchmark data sets
[1]. On the latter tasks, a large margin on all training points cannot be achieved
without adverse effects on the generalization error. This experimental observation
was supported by the study of [13] where the generalization error of ensemble methods
was bounded by the sum of the fraction of training points which have a margin
smaller than some value ae, say, plus a complexity term depending on the base hypotheses
and ae. While this bound can only capture part of what is going on in
practice, it nevertheless already conveys the message that in some cases it pays to
allow for some points which have a small margin, or are misclassified, if this leads
to a larger overall margin on the remaining points.
To cope with this problem, it was mandatory to construct regularized variants of
AdaBoost, which traded off the number of margin errors and the size of the margin

[9, 11]. This goal, however, had so far been achieved in a heuristic way by introducing
regularization parameters which have no immediate interpretation and which
cannot be adjusted easily.
The present paper addresses this problem in two ways. Primarily, it makes an algorithmic
 contribution to the problem of constructing regularized boosting algorithms.
However, compared to the previous efforts, it parameterizes the above trade-off in
a much more intuitive way: its only free parameter directly determines the fraction
of margin errors. This, in turn, is also appealing from a theoretical point of view,
since it involves a parameter which controls a quantity that plays a crucial role in
the generalization error bounds (cf. also [9, 13]). Furthermore, it allows the user
to roughly specify this parameter once a reasonable estimate of the expected error
(possibly from other studies) can be obtained, thus reducing the training time.

2 Boosting and the Linear Programming Solution

Before deriving a new algorithm, we briefly discuss the properties of the solution
generated by standard AdaBoost and, closely related, Arc-GV [2], and show the
relation to a linear programming (LP) solution over the class of base hypotheses G.

Let fg t (x) : t = 1; : : : ; Tg be a sequence of hypotheses and ff = [ff 1 : : : ff T ] their
weights satisfying ff t  0. The hypotheses g t are elements of a hypotheses class

G = fg : x 7! [\Gamma1; 1]g, which is defined by a base learning algorithm.
The ensemble generates the label which is the weighted majority of the votes by
sign(f(x)) where f(x) =

X

t

ff t

kffk 1

g t (x): (1)
In order to express that f and therefore also the margin ae depend on ff and for ease
of notation we define

ae(z; ff) := yf(x) where z := (x; y) and f is defined as in (1). (2)
Likewise we use the normalized margin:

ae(ff) := min

1im

ae(z i ; ff) ; (3)
Ensemble learning methods have to find both, the hypotheses g t 2 G used for the
combination and their weights ff. In the following we will consider only AdaBoost
algorithms (including Arcing). For more details see e.g. [4, 2]. The main idea of
AdaBoost is to introduce weights w t (z i ) on the training patterns. They are used to
control the importance of each single pattern in learning a new hypothesis (i.e. while
repeatedly running the base algorithm). Training patterns that are difficult to learn
(which are misclassified repeatedly) become more important.
The minimization objective of AdaBoost can be expressed in terms of margins as

G(ff) :=

m

X

i=1

exp(\Gammakffk 1 ae(z i ; ff)) : (4)
In every iteration, AdaBoost tries to minimize this error by a stepwise maximization
of the margin. It is widely believed that AdaBoost tries to maximize the smallest
margin on the training set [2, 5, 3, 13, 11]. Strictly speaking, however, a general
proof is missing. It would imply that AdaBoost asymptotically approximates (up to
scaling) the solution of the following linear programming problem over the complete
hypothesis set G (cf. [7], assuming a finite number of basis hypotheses):
maximize ae

subject to ae(z i ; ff)  ae for all 1  i  m
ff t ; ae  0 for all 1  t  jGj
kffk 1 = 1
(5)

Since such a linear program cannot be solved exactly for a infinite hypothesis set
in general, it is interesting to analyze approximation algorithms for this kind of
problems.
Breiman [2] proposed a modification of AdaBoost -- Arc-GV -- making it possible
to show the asymptotic convergence of ae(ff

t

) to the global solution ae

lp

:

Theorem 1 (Breiman [2]). Choose ff t in each iteration as

ff t := argmin

ff2[0;1]

X

i

exp

\Theta

\Gammakff

t

k 1

\Gamma

ae(z i ; ff

t

) \Gamma ae(ff

t\Gamma1

)

\Delta

; (6)

and assume that the base learner always finds the hypothesis g 2 G which minimizes
the weighted training error with respect to the weights. Then

lim

t!1

ae(ff

t

) = ae

lp

:

Note that the algorithm above can be derived from the modified error function

G gv (ff

t

) :=

X

i

exp

\Theta

\Gammakff

t

k 1

\Gamma

ae(z i ; ff

t

) \Gamma ae(ff

t\Gamma1

)

\Delta

: (7)
The question one might ask now is whether to use AdaBoost or rather Arc-GV
in practice. Does Arc-GV converge fast enough to benefit from its asymptotic
properties? In [12] we conducted experiments to investigate this question. We
empirically found that (a) AdaBoost has problems finding the optimal combination
if ae

lp

! 0, (b) Arc-GV's convergence does not depend on ae

lp

, and (c) for ae

lp

? 0,
AdaBoost usually converges to the maximum margin solution slightly faster than
Arc-GV. Observation (a) becomes clear from (4): G(ff) will not converge to 0 and

kffk 1 can be bounded by some value. Thus the asymptotic case cannot be reached,
whereas for Arc-GV the optimum is always found.
Moreover, the number of iterations necessary to converge to a good solution seems to
be reasonable, but for a near optimal solution the number of iterations is rather high.
This implies that for real world hypothesis sets, the number of iterations needed
to find an almost optimal solution can become prohibitive, but we conjecture that
in practice a reasonably good approximation to the optimum is provided by both
AdaBoost and Arc-GV.

3 -Algorithms

For the LP-AdaBoost [7] approach it has been shown for noisy problems that the
generalization performance is usually not as good as the one of AdaBoost [7, 2, 11].
From Theorem 5 in [13] (cf. Theorem 3 on page 5) this fact becomes clear, as
the minimum of the right hand side of inequality (cf. (13)) need not necessarily be
achieved with a maximum margin. We now propose an algorithm to directly control
the number of margin errors and therefore also the contribution of both terms in the
inequality separately (cf. Theorem 3). We first consider a small hypothesis class
and end up with a linear program -- -LP-AdaBoost. In subsection 3.2 we then
combine this algorithm with the ideas from section 2 and get a new algorithm --

-Arc -- which approximates the -LP solution.

3.1 -LP-AdaBoost

Let us consider the case where we are given a (finite) set G = fg : x 7! [\Gamma1; 1]g of T

hypotheses. To find the coefficients ff for the combined hypothesis f(x) we extend
the LP-AdaBoost algorithm [7, 11] by incorporating the parameter  [15] and solve
the following linear optimization problem:
maximize ae \Gamma

1

m

P m
i=1  i

subject to ae(z i ; ff)  ae \Gamma  i for all 1  i  m
 i ; ff t ; ae  0 for all 1  t  T and 1  i  m

kffk 1 = 1
(8)

This algorithm does not force all margins to be beyond zero and we get a soft
margin classification (cf. SVMs) with a regularization constant

1

m

. The following
proposition shows that  has an immediate interpretation:

Proposition 2 (Ratsch et al. [12]). Suppose we run the algorithm given in (8)
on some data with the resulting optimal ae ? 0. Then
1.  upper bounds the fraction of margin errors.
2. 1 \Gamma  upper bounds the fraction of patterns with margin larger than ae.

Since the slack variables  i only enter the cost function linearly, their absolute size
is not important. Loosely speaking, this is due to the fact that for the optimum
of the primal objective function, only derivatives wrt. the primal variables matter,
and the derivative of a linear function is constant.
In the case of SVMs [14], where the hypotheses can be thought of as vectors in
some feature space, this statement can be translated into a precise rule for distorting
training patterns without changing the solution: we can move them locally
orthogonal to a separating hyperplane. This yields a desirable robustness property.
Note that the algorithm essentially depends on the number of outliers, not on the
size of the error [15].

3.2 The -Arc Algorithm

Suppose we have a very large (but finite) base hypothesis class G. Then it is difficult
to solve (8) as (5) directly. To this end, we propose a new algorithm -- -Arc -- that
approximates the solution of (8).
The optimal ae for fixed margins ae(z i ; ff) in (8) can be written as
ae  (ff) := argmax

ae2[0;1]

/

ae \Gamma

1

m

m

X

i=1

(ae \Gamma ae(z i ; ff)) +

!

: (9)
where () + := max(; 0). Setting  i := (ae  (ff) \Gamma ae(z i ; ff)) + and subtracting

1

m

P m
i=1  i from the resulting inequality on both sides yields (for all 1  i  m)
ae(z i ; ff) +  i \Gamma

1

m

m

X

i=1

 i  ae  (ff) \Gamma

1

m

m

X

i=1

 i : (10)
Two more substitutions are needed to transform the problem into one which can
be solved by the AdaBoost algorithm. In particular we have to get rid of the slack
variables  i again by absorbing them into quantities similar to ae(z i ; ff) and ae(ff).

This works as follows: on the right hand side of (10) we have the objective function
(cf. (8)) and on the left hand side a term that depends nonlinearly on ff. Defining
~ ae  (ff) := ae  (ff) \Gamma

1

m

m

X

i=1

 i and ~ ae  (z i ; ff) := ae(z i ; ff) +  i \Gamma

1

m

m

X

i=1

 i ; (11)
which we substitute for ae(ff) and ae(z; ff) in (5), respectively, we obtain a new
optimization problem. Note that ~ ae  (ff) and ~ ae  (z i ; ff) play the role of a corrected

or virtual margin. We obtain a nonlinear min-max problem
maximize ~ ae(ff)

subject to ~ ae(z i ; ff)  ~ ae(ff) for all 1  i  m
ff t  0 for all 1  t  T

kffk 1 = 1

; (12)
which Arc-GV can solve approximately (cf. section 2). Hence, by replacing the margin
 ae(z; ff) by ~ ae(z; ff) in equation (4) and the other formulas for Arc-GV (cf. [2]),

we obtain a new algorithm which we refer to as -Arc.

We can now state interesting properties for -Arc by using Theorem 5 of [13] that
bounds the generalization error R(f) for ensemble methods. In our case R ae (f)  

by construction (i.e. the number of patterns with a margin smaller than ae, cf. Proposition
2), thus we get the following simple reformulation of this bound:

Theorem 3. Let p(x; y) be a distribution over X \Theta [\Gamma1; 1], and let X be a sample
of m examples chosen iid according to p. Suppose the base-hypothesis space G has
VC dimension h, and let ffi ? 0. Then with probability at least 1 \Gamma ffi over the
random choice of the training set X, Y , every function f generated by -Arc, where

 2 (0; 1) and ae  ? 0, satisfies the following bound:
R(f)   +

s

c
m

`

h log

2

(m=h)

ae

2



+ log

`

1

ffi

"

: (13)
So, for minimizing the right hand side we can tradeoff between the first and the
second term by controlling an easily interpretable regularization parameter .

4 Experiments

We show a set of toy experiments to illustrate the general behavior of -Arc. As
base hypothesis class G we use the RBF networks of [11], and as data a two-class
problem generated from several 2D Gauss blobs (cf. Banana shape dataset from

http://www.first.gmd.de/~data/banana.html.). We obtain the following results:

ffl -Arc leads to approximately m patterns that are effectively used in
the training of the base learner: Figure 1 (left) shows the fraction
of patterns that have high average weights during the learning process
(i.e.

P T
t=1 w t (z i ) ? 1=2m). We find that the number of the latter increases
(almost) linearly with . This follows from (11) as the (soft) margin of
patterns with ae(z; ff) ! ae  is set to ae  and the weight of those patterns will
be the same.

ffl The (estimated) test error, averaged over 10 training sets, exhibits a rather
flat minimum in  (Figure 1 (lower)). This indicates that just as for -

SVMs, where corresponding results have been obtained,  is a well-behaved
parameter in the sense that a slight misadjustment it is not harmful.

ffl -Arc leads to the fraction  of margin errors (cf. dashed line in Figure 1)
exactly as predicted in Proposition 2.

ffl Finally, a good value of  can already be inferred from prior knowledge of
the expected error. Setting it to a value similar to the latter provides a
good starting point for further optimization (cf. Theorem 3).
Note that for  = 1, we recover the Bagging algorithm (if we used bootstrap
samples), as the weights of all patterns will be the same (w t (z i ) = 1=m for all

i = 1; : : : ; m) and also the hypothesis weights will be constant (ff t  1=T for all

t = 1; : : : ; T ).
Finally, we present a small comparison on ten benchmark
data sets obtained from the UCI [1] benchmark repository
(cf. http://ida.first.gmd.de/~raetsch/data/benchmarks.html). We analyze
the performance of single RBF networks, AdaBoost, -Arc and RBF-SVMs.
For AdaBoost and -Arc we use RBF networks [11] as base hypothesis. The
model parameters of RBF (number of centers etc.), -Arc () and SVMs (oe; C) are
optimized using 5-fold cross-validation. More details on the experimental setup can

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.2
0.4
0.6
0.8
1
n

training error
number of important
patterns
number of
margin errors
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.11
0.12
0.13
0.14
0.15
0.16
n

E

Bagging n-Arc for optimal n

Arc-GV
Figure 1: Toy experiment (oe = 0): the left graph shows the average fraction of important

patterns, the av. fraction of margin errors and the av. training error for different values
of the regularization constant  for -Arc. The right graph shows the corresponding
generalization error. In both cases, the parameter  allows us to reduce the test errors
to values much lower than for the hard margin algorithm (for  = 0 we recover ArcGV
/AdaBoost, and for  = 1 we get Bagging.)

be found in [11]. Fig. 1 shows the generalization error estimates (after averaging
over 100 realizations of the data sets) and the confidence interval. The results
of the best classifier and the classifiers that are not significantly worse are set in
bold face. To test the significance, we used a t-test (p = 80%). On eight out of
the ten data sets, -Arc performs significantly better than AdaBoost. This clearly
shows the superior performance of -Arc for noisy data sets and supports this soft
margin approach for AdaBoost. Furthermore, we find comparable performances
for -Arc and SVMs. In three cases the SVM performs better and in two cases

-Arc performs best. Summarizing, AdaBoost is useful for low noise cases, where
the classes are separable. -Arc extends the applicability of boosting to problems
that are difficult to separate and should be applied if the data are noisy.

5 Conclusion

We analyzed the AdaBoost algorithm and found that Arc-GV and AdaBoost are
efficient for approximating the solution of non-linear min-max problems over huge
hypothesis classes. We re-parameterized the LP Reg -AdaBoost algorithm (cf. [7, 11])
and introduced a new regularization constant  that controls the fraction of patterns
inside the margin area. The new parameter is highly intuitive and has to be
optimized only on a fixed interval [0; 1].
Using the fact that Arc-GV can approximately solve min-max problems, we found a
formulation of Arc-GV -- -Arc -- that implements the -idea for Boosting by defining
an appropriate soft margin. The present paper extends previous work on regularizing
boosting (DOOM [9], AdaBoost Reg [11]) and shows the utility and flexibility
of the soft margin approach for AdaBoost.
RBF AB -Arc SVM
Banana 10.8 \Sigma 0.06 12.3 \Sigma 0.07 10.6 \Sigma 0.05 11.5 \Sigma 0.07
B.Cancer 27.6 \Sigma 0.47 30.4 \Sigma 0.47 25.8 \Sigma 0.46 26.0 \Sigma 0.47

Diabetes 24.3 \Sigma 0.19 26.5 \Sigma 0.23 23.7 \Sigma 0.20 23.5 \Sigma 0.17

German 24.7 \Sigma 0.24 27.5 \Sigma 0.25 24.4 \Sigma 0.22 23.6 \Sigma 0.21

Heart 17.6 \Sigma 0.33 20.3 \Sigma 0.34 16.5 \Sigma 0.36 16.0 \Sigma 0.33

Ringnorm 1.7 \Sigma 0.02 1.9 \Sigma 0.03 1.7 \Sigma 0.02 1.7 \Sigma 0.01

F.Sonar 34.4 \Sigma 0.20 35.7 \Sigma 0.18 34.4 \Sigma 0.19 32.4 \Sigma 0.18

Thyroid 4.5 \Sigma 0.21 4.4 \Sigma 0.22 4.4 \Sigma 0.22 4.8 \Sigma 0.22
Titanic 23.3 \Sigma 0.13 22.6 \Sigma 0.12 23.0 \Sigma 0.14 22.4 \Sigma 0.10

Waveform 10.7 \Sigma 0.11 10.8 \Sigma 0.06 10.0 \Sigma 0.07 9.9 \Sigma 0.04

Table 1: Generalization error estimates and confidence intervals. The best classifiers for a
particular data set are marked in bold face (see text).

We found empirically that the generalization performance in -Arc depends only
slightly on the choice of the regularization constant. This makes model selection
(e.g. via cross-validation) easier and faster.
Future work will study the detailed regularization properties of the regularized versions
of AdaBoost, in particular in comparison to -LP Support Vector Machines.

Acknowledgments: Partial funding from DFG grant (Ja 379/52) is gratefully
acknowledged. This work was done while AS and BS were at GMD FIRST.

References

[1] C. Blake, E. Keogh, and C. J. Merz. UCI repository of machine learning databases,
1998. http://www.ics.uci.edu/mlearn/MLRepository.html.
[2] L. Breiman. Prediction games and arcing algorithms. Technical Report 504, Statistics
Department, University of California, December 1997.
[3] M. Frean and T. Downs. A simple cost function for boosting. Technical report, Dept.
of Computer Science and Electrical Eng., University of Queensland, 1998.
[4] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning
and an application to boosting. In Computational Learning Theory: Eurocolt '95,

pages 23--37. Springer-Verlag, 1995.
[5] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning
and an application to boosting. J. of Comp. & Syst. Sc., 55(1):119--139, 1997.
[6] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical
view of boosting. Technical report, Stanford University, 1998.
[7] A. Grove and D. Schuurmans. Boosting in the limit: Maximizing the margin of
learned ensembles. In Proc. of the 15th Nat. Conf. on AI, pages 692--699, 1998.
[8] Y. LeCun, L. D. Jackel, L. Bottou, C. Cortes, J. S. Denker, H. Drucker, I. Guyon,
U. A. Muller, E. Sackinger, P. Simard, and V. Vapnik. Learning algorithms for
classification: A comparison on handwritten digit recognition. Neural Networks, pages
261--276, 1995.
[9] L. Mason, P. L. Bartlett, and J. Baxter. Improved generalization through explicit
optimization of margins. Machine Learning, 1999. to appear.
[10] J. R. Quinlan. Boosting first-order learning (invited lecture). Lecture Notes in Computer
 Science, 1160:143, 1996.
[11] G. Ratsch, T. Onoda, and K.-R. Muller. Soft margins for AdaBoost. Technical Report
NC-TR-1998-021, Department of Computer Science, Royal Holloway, University of
London, Egham, UK, 1998. To appear in Machine Learning.
[12] G. Ratsch, B. Schokopf, A. Smola, S. Mika, T. Onoda, and K.-R. Muller. Robust
ensemble learning. In A.J. Smola, P.L. Bartlett, B. Scholkopf, and D. Schuurmans,
editors, Advances in LMC, pages 207--219. MIT Press, Cambridge, MA, 1999.
[13] R. Schapire, Y. Freund, P. L. Bartlett, and W. Sun Lee. Boosting the margin: A
new explanation for the effectiveness of voting methods. Annals of Statistics, 1998.
(Earlier appeared in: D. H. Fisher, Jr. (ed.), Proc. ICML97, M. Kaufmann).
[14] B. Scholkopf, C. J. C. Burges, and A. J. Smola. Advances in Kernel Methods ---
Support Vector Learning. MIT Press, Cambridge, MA, 1999.
[15] B. Scholkopf, A. Smola, R. C. Williamson, and P. L. Bartlett. New support vector
algorithms. Neural Computation, 12:1083 -- 1121, 2000.
[16] H. Schwenk and Y. Bengio. Training methods for adaptive boosting of neural networks.
In Michael I. Jordan, Michael J. Kearns, and Sara A. Solla, editors, Advances
in Neural Inf. Processing Systems, volume 10. The MIT Press, 1998.
[17] V. Vapnik. The Nature of Statistical Learning Theory. Springer Verlag, New York,
1995.

